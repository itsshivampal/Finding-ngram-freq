{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemming(text):\n",
    "    porter_stemmer  = PorterStemmer()\n",
    "    word_tokens = text.split(\" \")\n",
    "    words = [porter_stemmer.stem(word) for word in word_tokens]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def wordnet_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = text.split(\" \")\n",
    "    words = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    sentence = \" \".join(filtered_sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    new_text = \"\"\n",
    "    punctuations = \"!\\\"#$%&()*+-.,:;<=>?@[\\]^_{|}~\"\n",
    "    for ch in text:\n",
    "        if ch not in punctuations:\n",
    "            new_text += ch\n",
    "        else:\n",
    "            new_text += \" \"\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def text_cleaning(content):\n",
    "    content = str(content)\n",
    "    content = content.lower()\n",
    "    content = re.sub(r'\\d+', '', content)\n",
    "    content = remove_punctuations(content)\n",
    "    content = remove_stopwords(content)\n",
    "#     content = porter_stemming(content)\n",
    "#     content = wordnet_lemmatization(content)\n",
    "    content = content.strip()\n",
    "    return content\n",
    "\n",
    "def messageCleaning(message):\n",
    "    cleanMsg = []\n",
    "    for msg in message:\n",
    "        text = text_cleaning(msg)\n",
    "        cleanMsg.append(text)\n",
    "    return cleanMsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgram(msg, n):\n",
    "    ngram = []\n",
    "    stringArray = ngrams(msg, n)\n",
    "    for grams in stringArray:\n",
    "        ngram.append(\" \".join(grams))\n",
    "    return ngram\n",
    "\n",
    "def getNgramArray(message, n):\n",
    "    ngramArray = []\n",
    "    for msg in message:\n",
    "#         msg = text_cleaning(msg)\n",
    "        msg = word_tokenize(msg)\n",
    "        ngramArray += getNgram(msg, n)\n",
    "    return ngramArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(msgFreqDist, name):\n",
    "    df_fdist = pd.DataFrame.from_dict(msgFreqDist, orient='index')\n",
    "    df_fdist.columns = ['Frequency']\n",
    "    df_fdist.index.name = 'Word'\n",
    "    df_fdist.to_csv(name)\n",
    "    return df_fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGramCollection(n):\n",
    "    onegramCollection = []\n",
    "    onegramCollection += getNgramArray(msg1, n)\n",
    "    onegramCollection += getNgramArray(msg2, n)\n",
    "    onegramCollection += getNgramArray(msg3, n)\n",
    "    onegramCollection += getNgramArray(msg4, n)\n",
    "    return onegramCollection\n",
    "\n",
    "def saveFregDist(n, name):\n",
    "    gramCollection = getGramCollection(n)\n",
    "    msgFreqDist = nltk.FreqDist(gramCollection)\n",
    "    df = saveData(msgFreqDist, name)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data from CSV files\n",
    "msg1 = pd.read_csv('Data/May2020-NotesMsg.csv')['msg'].values\n",
    "msg1 = messageCleaning(msg1)\n",
    "msg2 = pd.read_csv('Data/June2020-NotesMsg.csv')['msg'].values\n",
    "msg2 = messageCleaning(msg2)\n",
    "msg3 = pd.read_csv('Data/July2020-NotesMsg.csv')['msg'].values\n",
    "msg3 = messageCleaning(msg3)\n",
    "msg4 = pd.read_csv('Data/August2020-NotesMsg.csv')['msg'].values\n",
    "msg4 = messageCleaning(msg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Frequency\n",
      "Word                                 \n",
      "जुलाई                            1150\n",
      "से                               6772\n",
      "सभी                              3094\n",
      "स्कूल                            1525\n",
      "खुलेंगे                             1\n",
      "...                               ...\n",
      "com/z/ittoe                         1\n",
      "com/z/jswzp/efbadeadcafbfc          1\n",
      "com/z/jswzp                         1\n",
      "com/z/julc/efbadeadcafbfc           1\n",
      "com/z/julc                          1\n",
      "\n",
      "[284267 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "name = 'singleWordFreq.csv'\n",
    "df1 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Frequency\n",
      "Word                                         \n",
      "जुलाई से                                   87\n",
      "से सभी                                     48\n",
      "सभी स्कूल                                  18\n",
      "स्कूल खुलेंगे                               1\n",
      "answer following                          431\n",
      "...                                       ...\n",
      "theuolo com/z/ittoe                         1\n",
      "theuolo com/z/jswzp/efbadeadcafbfc          1\n",
      "theuolo com/z/jswzp                         1\n",
      "theuolo com/z/julc/efbadeadcafbfc           1\n",
      "theuolo com/z/julc                          1\n",
      "\n",
      "[963711 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "name = 'twoWordFreq.csv'\n",
    "df2 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Frequency\n",
      "Word                                                 \n",
      "जुलाई से सभी                                        1\n",
      "से सभी स्कूल                                        1\n",
      "सभी स्कूल खुलेंगे                                   1\n",
      "hello thaslima welcome                              1\n",
      "thaslima welcome uolo                               2\n",
      "...                                               ...\n",
      "service theuolo com/z/ittoe                         1\n",
      "service theuolo com/z/jswzp/efbadeadcafbfc          1\n",
      "service theuolo com/z/jswzp                         1\n",
      "service theuolo com/z/julc/efbadeadcafbfc           1\n",
      "service theuolo com/z/julc                          1\n",
      "\n",
      "[1580652 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "name = 'threeWordFreq.csv'\n",
    "df3 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-gram distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Frequency\n",
      "Word                                                      \n",
      "जुलाई से सभी स्कूल                                       1\n",
      "से सभी स्कूल खुलेंगे                                     1\n",
      "hello thaslima welcome uolo                              1\n",
      "thaslima welcome uolo congratulations                    2\n",
      "welcome uolo congratulations winning                 22525\n",
      "...                                                    ...\n",
      "//vc service theuolo com/z/ittoe                         1\n",
      "//vc service theuolo com/z/jswzp/efbadeadcafbfc          1\n",
      "//vc service theuolo com/z/jswzp                         1\n",
      "//vc service theuolo com/z/julc/efbadeadcafbfc           1\n",
      "//vc service theuolo com/z/julc                          1\n",
      "\n",
      "[1851726 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "name = 'fourWordFreq.csv'\n",
    "df4 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Frequency\n",
      "Word                                                         \n",
      "जुलाई से सभी स्कूल खुलेंगे                                  1\n",
      "hello thaslima welcome uolo congratulations                 1\n",
      "thaslima welcome uolo congratulations winning               2\n",
      "welcome uolo congratulations winning gift               22525\n",
      "uolo congratulations winning gift voucher               22525\n",
      "...                                                       ...\n",
      "https //vc service theuolo com/z/ittoe                      1\n",
      "https //vc service theuolo com/z/jswzp/efbadead...          1\n",
      "https //vc service theuolo com/z/jswzp                      1\n",
      "https //vc service theuolo com/z/julc/efbadeadc...          1\n",
      "https //vc service theuolo com/z/julc                       1\n",
      "\n",
      "[1957487 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "name = 'fiveWordFreq.csv'\n",
    "df5 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Frequency\n",
      "Word                                                         \n",
      "hello thaslima welcome uolo congratulations win...          1\n",
      "thaslima welcome uolo congratulations winning gift          2\n",
      "welcome uolo congratulations winning gift voucher       22525\n",
      "uolo congratulations winning gift voucher celeb...      22525\n",
      "congratulations winning gift voucher celebratin...      22525\n",
      "...                                                       ...\n",
      "link https //vc service theuolo com/z/jswzp/efb...          1\n",
      "aug https //vc service theuolo com/z/jswzp                  1\n",
      "link https //vc service theuolo com/z/julc/efba...          1\n",
      "aug https //vc service theuolo com/z/julc                   1\n",
      "st mid term marks feedback homework                         1\n",
      "\n",
      "[2025310 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "name = 'sixWordFreq.csv'\n",
    "df6 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Frequency\n",
      "Word                                                         \n",
      "hello thaslima welcome uolo congratulations win...          1\n",
      "thaslima welcome uolo congratulations winning g...          2\n",
      "welcome uolo congratulations winning gift vouch...      22525\n",
      "uolo congratulations winning gift voucher celeb...      22525\n",
      "congratulations winning gift voucher celebratin...      22525\n",
      "...                                                       ...\n",
      "clicking link https //vc service theuolo com/z/...          1\n",
      "fri aug https //vc service theuolo com/z/jswzp              1\n",
      "clicking link https //vc service theuolo com/z/...          1\n",
      "fri aug https //vc service theuolo com/z/julc               1\n",
      "computer st mid term marks feedback homework                1\n",
      "\n",
      "[2013075 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "name = 'sevenWordFreq.csv'\n",
    "df7 = saveFregDist(n, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
